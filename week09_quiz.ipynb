{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250888a9-6250-4bbf-a2f5-2fee0b4e2044",
   "metadata": {},
   "source": [
    "- Question\n",
    "\n",
    "  - 영화리뷰 긍/부정 예측 Estimator pipeline을 테스트 (trainRatio=0.8)\n",
    "  - Word2Vec의 파라미터 vectorSize를 5, 10, 20, 40으로 바꾸며 정확도를 측정하여 출력\n",
    "  - 매뉴얼 및 웹검색을 통해 문제해결!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee11dfa-8326-407a-ab58-3a1ee23533ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 17:09:13 WARN Utils: Your hostname, bagdoyeong-ui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 192.168.0.13 instead (on interface en0)\n",
      "22/10/27 17:09:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 17:09:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/27 17:09:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/27 17:09:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('my app').master('local[8]').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54c063f-ade5-4f4e-880c-231f0280d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, Word2Vec, VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb5cb4ba-7bc4-46c4-9d88-9eaa837e9a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('label', FloatType(), True)\n",
    "])\n",
    "\n",
    "data = spark.read.format('csv').option('escape', \"\\\"\").option('header', 'true').schema(schema).load('input_week09/imdb-review-sentiment.csv')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dac8b83-927b-44bf-8a83-4db82171b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|I grew up (b. 196...|  0.0|\n",
      "|When I put this m...|  0.0|\n",
      "|Why do people who...|  0.0|\n",
      "|Even though I hav...|  0.0|\n",
      "|Im a die hard Dad...|  1.0|\n",
      "|A terrible movie ...|  0.0|\n",
      "|Finally watched t...|  1.0|\n",
      "|I caught this fil...|  0.0|\n",
      "|It may be the rem...|  1.0|\n",
      "|My Super Ex Girlf...|  1.0|\n",
      "|I can't believe p...|  1.0|\n",
      "|If you haven't se...|  0.0|\n",
      "|I have always bee...|  1.0|\n",
      "|Greg Davis and Br...|  0.0|\n",
      "|A half-hearted at...|  0.0|\n",
      "|If you want a fun...|  1.0|\n",
      "|I really wanted t...|  1.0|\n",
      "|The main problem ...|  0.0|\n",
      "|The folks at Disn...|  0.0|\n",
      "|A friend told me ...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('text', 'label').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e735e44-9140-445c-9816-36cabb456f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'the', 'she', 'he', 'have', 'not']\n",
    "\n",
    "# df = spark.createDataFrame([\n",
    "#     ('This movie was so poorly written and directed I fell asleep 30minutes through the movie.', 0),\n",
    "#     ('The most interesting thing about Miryang (Secret Sunshine) is the actores.', 1),\n",
    "#     ('William Hurt may not be an American matinee idol anymorem but he still has pretty good talent.', 1)\n",
    "# ], ['text', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe684c71-e044-4602-b9e3-61fb3f4e783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveStopWordsAndSpecialCharacters(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super(RemoveStopWordsAndSpecialCharacters, self).__init__()\n",
    "        self.stopwords = Param(self, 'stopwords', '')\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "        \n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "    \n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "        \n",
    "        def f(s):\n",
    "            return [ ''.join(e for e in token if e.isalnum()) for token in s if token not in stopwords]\n",
    "        \n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b3e87bbc-3a85-40d6-9ff9-2f93a43e6eb8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "cleaning = RemoveStopWordsAndSpecialCharacters(inputCol='words', outputCol='clean_words', stopwords=stopwords)\n",
    "# hashingTF = HashingTF(inputCol='clean_words', outputCol='tf')\n",
    "w2v = Word2Vec(vectorSize=10, inputCol='clean_words', outputCol='w2v', minCount=1, maxIter=1, )\n",
    "asm = VectorAssembler(inputCols = [w2v.getOutputCol()], outputCol='features')\n",
    "svm = LinearSVC(labelCol='label', maxIter=5)\n",
    "\n",
    "mypipeline = Pipeline(stages=[tokenizer, cleaning, w2v, asm, svm])\n",
    "df10 = mypipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1973cce1-9273-4f4e-a9d3-d690aa2fe1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "cleaning = RemoveStopWordsAndSpecialCharacters(inputCol='words', outputCol='clean_words', stopwords=stopwords)\n",
    "# hashingTF = HashingTF(inputCol='clean_words', outputCol='tf')\n",
    "w2v = Word2Vec(vectorSize=15, inputCol='clean_words', outputCol='w2v', minCount=1, maxIter=1, )\n",
    "asm = VectorAssembler(inputCols = [w2v.getOutputCol()], outputCol='features')\n",
    "svm = LinearSVC(labelCol='label', maxIter=5)\n",
    "\n",
    "mypipeline = Pipeline(stages=[tokenizer, cleaning, w2v, asm, svm])\n",
    "df15 = mypipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9276399b-366d-41be-82d8-1bebbe195b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "cleaning = RemoveStopWordsAndSpecialCharacters(inputCol='words', outputCol='clean_words', stopwords=stopwords)\n",
    "# hashingTF = HashingTF(inputCol='clean_words', outputCol='tf')\n",
    "w2v = Word2Vec(vectorSize=20, inputCol='clean_words', outputCol='w2v', minCount=1, maxIter=1, )\n",
    "asm = VectorAssembler(inputCols = [w2v.getOutputCol()], outputCol='features')\n",
    "svm = LinearSVC(labelCol='label', maxIter=5)\n",
    "\n",
    "mypipeline = Pipeline(stages=[tokenizer, cleaning, w2v, asm, svm])\n",
    "df20 = mypipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3ca8649f-370c-4a01-95a5-1b186b9994f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61.834999999999994"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size = 5\n",
    "\n",
    "df_result = df.withColumn('prediction', df['prediction'].cast(FloatType()))\n",
    "df_result.selectExpr('label == prediction').filter('(label = prediction) == True').count() / df_result.count() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2f213f6e-8df2-4a59-9a22-537720a380ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.22"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size = 10\n",
    "\n",
    "df_result = df10.withColumn('prediction', df10['prediction'].cast(FloatType()))\n",
    "df_result.selectExpr('label == prediction').filter('(label = prediction) == True').count() / df_result.count() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ef4d03e2-8886-4a4d-8604-15dd33599885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.0375"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size = 15\n",
    "\n",
    "df_result = df15.withColumn('prediction', df15['prediction'].cast(FloatType()))\n",
    "df_result.selectExpr('label == prediction').filter('(label = prediction) == True').count() / df_result.count() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12fc9007-5300-4fdf-a11d-4b41e83cb437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72.1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size = 20\n",
    "\n",
    "df_result = df20.withColumn('prediction', df20['prediction'].cast(FloatType()))\n",
    "df_result.selectExpr('label == prediction').filter('(label = prediction) == True').count() / df_result.count() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658ae4b-b4fc-43a8-a3f0-6eca66bea64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
