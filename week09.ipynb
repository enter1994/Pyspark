{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250888a9-6250-4bbf-a2f5-2fee0b4e2044",
   "metadata": {},
   "source": [
    "- Question\n",
    "\n",
    "  - 영화리뷰 긍/부정 예측 Estimator pipeline을 테스트 (trainRatio=0.8)\n",
    "  - Word2Vec의 파라미터 vectorSize를 5, 10, 20, 40으로 바꾸며 정확도를 측정하여 출력\n",
    "  - 매뉴얼 및 웹검색을 통해 문제해결!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee11dfa-8326-407a-ab58-3a1ee23533ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 16:41:51 WARN Utils: Your hostname, bagdoyeong-ui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 192.168.0.13 instead (on interface en0)\n",
      "22/10/27 16:41:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 16:41:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/27 16:41:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('my app').master('local').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54c063f-ade5-4f4e-880c-231f0280d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, Word2Vec, VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e735e44-9140-445c-9816-36cabb456f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'the', 'she', 'he', 'have', 'not']\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    ('This movie was so poorly written and directed I fell asleep 30minutes through the movie.', 0),\n",
    "    ('The most interesting thing about Miryang (Secret Sunshine) is the actores.', 1),\n",
    "    ('William Hurt may not be an American matinee idol anymorem but he still has pretty good talent.', 1)\n",
    "], ['text', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c83ce27-78e9-4c8d-b91e-08b22396e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                               words|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[this, movie, was, so, poorly, written, and, directed, i, fell, asleep, 30minutes, through, the, ...|\n",
      "|              [the, most, interesting, thing, about, miryang, (secret, sunshine), is, the, actores.]|\n",
      "|[william, hurt, may, not, be, an, american, matinee, idol, anymorem, but, he, still, has, pretty,...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "df = tokenizer.transform(df)\n",
    "df.select('words').show(5, truncate=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff64f6b7-00dc-46fe-88a5-93b80f333f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                            features|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|(262144,[19036,42181,52800,61318,90636,95889,99211,108541,112584,117491,121809,188235,210223,2199...|\n",
      "|(262144,[18700,61470,70065,95889,106841,124227,189082,195413,234706,259618],[1.0,1.0,1.0,2.0,1.0,...|\n",
      "|(262144,[5429,23071,25718,31536,33917,67846,68474,91192,113432,121981,132786,138836,143202,145207...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')\n",
    "df = hashingTF.transform(df)\n",
    "df.select('features').show(5, truncate=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a9bd92-1e0d-413e-be9c-5e5e82b40ddc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                         clean_words|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[this, movie, was, so, poorly, written, and, directed, fell, asleep, 30minutes, through, the, movie]|\n",
      "|                 [the, most, interesting, thing, about, miryang, secret, sunshine, is, the, actores]|\n",
      "|[william, hurt, may, be, an, american, matinee, idol, anymorem, but, still, has, pretty, good, ta...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# udf 이용하여 tranform\n",
    "def f(s):\n",
    "    return [ ''.join(e for e in token if e.isalnum()) for token in s if token not in stopwords ]\n",
    "\n",
    "func = udf(f, ArrayType(StringType()))\n",
    "df = df.withColumn('clean_words', func(df['words']))\n",
    "df.select('clean_words').show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe684c71-e044-4602-b9e3-61fb3f4e783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveStopWordsAndSpecialCharacters(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super(RemoveStopWordsAndSpecialCharacters, self).__init__()\n",
    "        self.stopwords = Param(self, 'stopwords', '')\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "        \n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "    \n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "        \n",
    "        def f(s):\n",
    "            return [ ''.join(e for e in token if e.isalnum()) for token in s if token not in stopwords]\n",
    "        \n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3e87bbc-3a85-40d6-9ff9-2f93a43e6eb8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 17:04:46 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 17:04:47 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 17:04:48 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/10/27 17:04:49 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/10/27 17:04:49 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/10/27 17:04:50 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/10/27 17:04:51 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/10/27 17:04:51 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/10/27 17:04:52 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/10/27 17:04:53 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 17:04:54 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "cleaning = RemoveStopWordsAndSpecialCharacters(inputCol='words', outputCol='clean_words', stopwords=stopwords)\n",
    "hashingTF = HashingTF(inputCol='clean_words', outputCol='tf')\n",
    "w2v = Word2Vec(vectorSize=2, inputCol='clean_words', outputCol='w2v', minCount=1, maxIter=10)\n",
    "asm = VectorAssembler(inputCols = [hashingTF.getOutputCol(), w2v.getOutputCol()], outputCol='features')\n",
    "svm = LinearSVC(labelCol='sentiment')\n",
    "\n",
    "mypipeline = Pipeline(stages=[tokenizer, cleaning, hashingTF, w2v, asm, svm])\n",
    "df = mypipeline.fit(df).transform(df)\n",
    "df.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867719a-2e95-4d99-a3d0-bb0e8c4b06b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44c8d6-84d1-41b4-8a12-71688c72b709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
